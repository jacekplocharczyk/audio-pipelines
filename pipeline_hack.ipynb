{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__Python VERSION: 3.7.5 (default, Oct 25 2019, 15:51:11) \n",
      "[GCC 7.3.0]\n",
      "__pyTorch VERSION: 1.2.0\n",
      "__CUDA VERSION\n",
      "/bin/sh: 1: nvcc: not found\n",
      "__CUDNN VERSION: 7301\n",
      "__Number CUDA Devices: 2\n",
      "__Devices\n",
      "Active CUDA Device: GPU 0\n",
      "Available devices  2\n",
      "Current cuda device  0\n",
      "3.7.5 (default, Oct 25 2019, 15:51:11) \n",
      "[GCC 7.3.0]\n",
      "10.5\n",
      "svmem(total=67477729280, available=31186391040, percent=53.8, used=35052150784, free=8833298432, active=39937445888, inactive=12108967936, buffers=420139008, cached=23172141056, shared=506003456, slab=5229731840)\n",
      "memory GB: 0.24679946899414062\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import sys\n",
    "import torch\n",
    "from torch.utils.data.dataset import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "\n",
    "from sklearn import model_selection\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import roc_auc_score, log_loss, roc_auc_score, roc_curve, auc\n",
    "from sklearn.model_selection import StratifiedKFold, ShuffleSplit, cross_val_score, train_test_split\n",
    "\n",
    "print('__Python VERSION:', sys.version)\n",
    "print('__pyTorch VERSION:', torch.__version__)\n",
    "print('__CUDA VERSION')\n",
    "from subprocess import call\n",
    "# call([\"nvcc\", \"--version\"]) does not work\n",
    "! nvcc --version\n",
    "print('__CUDNN VERSION:', torch.backends.cudnn.version())\n",
    "print('__Number CUDA Devices:', torch.cuda.device_count())\n",
    "print('__Devices')\n",
    "# call([\"nvidia-smi\", \"--format=csv\", \"--query-gpu=index,name,driver_version,memory.total,memory.used,memory.free\"])\n",
    "print('Active CUDA Device: GPU', torch.cuda.current_device())\n",
    "\n",
    "print ('Available devices ', torch.cuda.device_count())\n",
    "print ('Current cuda device ', torch.cuda.current_device())\n",
    "\n",
    "import numpy\n",
    "import numpy as np\n",
    "\n",
    "use_cuda = torch.cuda.is_available()\n",
    "FloatTensor = torch.cuda.FloatTensor if use_cuda else torch.FloatTensor\n",
    "LongTensor = torch.cuda.LongTensor if use_cuda else torch.LongTensor\n",
    "Tensor = FloatTensor\n",
    "\n",
    "import pandas\n",
    "import pandas as pd\n",
    "\n",
    "import logging\n",
    "handler=logging.basicConfig(level=logging.INFO)\n",
    "lgr = logging.getLogger(__name__)\n",
    "%matplotlib inline\n",
    "\n",
    "# !pip install psutil\n",
    "import psutil\n",
    "import os\n",
    "def cpuStats():\n",
    "        print(sys.version)\n",
    "        print(psutil.cpu_percent())\n",
    "        print(psutil.virtual_memory())  # physical memory usage\n",
    "        pid = os.getpid()\n",
    "        py = psutil.Process(pid)\n",
    "        memoryUse = py.memory_info()[0] / 2. ** 30  # memory use in GB...I think\n",
    "        print('memory GB:', memoryUse)\n",
    "\n",
    "cpuStats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_cuda = torch.cuda.is_available()\n",
    "# use_cuda = False\n",
    "\n",
    "FloatTensor = torch.cuda.FloatTensor if use_cuda else torch.FloatTensor\n",
    "LongTensor = torch.cuda.LongTensor if use_cuda else torch.LongTensor\n",
    "Tensor = FloatTensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/projects/audio-pipelines/data\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyps = pd.read_csv(\"sets/full_set/hyps\", names=['audio_id'])\n",
    "hyps[['audio_id','hyps']] = hyps[\"audio_id\"].str.split(\" \", 1, expand=True)\n",
    "\n",
    "targets = pd.read_csv(\"sets/full_set/targets\", names=['audio_id'])\n",
    "targets[['audio_id','target']] = targets[\"audio_id\"].str.split(\" \", 1, expand=True)\n",
    "\n",
    "text = pd.read_csv(\"sets/full_set/text\", names=['audio_id'])\n",
    "text[['audio_id','text']] = text[\"audio_id\"].str.split(\" \", 1, expand=True)\n",
    "\n",
    "speakers = pd.read_csv(\"sets/full_set/utt2spk\", names=['audio_id'])\n",
    "speakers[['audio_id','speaker']] = speakers[\"audio_id\"].str.split(\" \", 1, expand=True)\n",
    "\n",
    "images = pd.read_csv(\"sets/full_set/wav.scp\", names=['audio_id'])\n",
    "images[['audio_id','audio_path']] = images[\"audio_id\"].str.split(\" \", 1, expand=True)\n",
    "\n",
    "dataset = pd.merge(hyps, text, how=\"left\")\n",
    "dataset = pd.merge(dataset, speakers, how=\"left\")\n",
    "dataset = pd.merge(dataset, images, how=\"left\")\n",
    "dataset = pd.merge(dataset, targets, how=\"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['image_path'] = dataset['audio_path'].apply(lambda x : x[:-4] + \".png\")\n",
    "def remove_absolute(string, prefix='/home/raznem/projects/audio-pipelines/data/'):\n",
    "    if string.startswith(prefix):\n",
    "        string = string[len(prefix):]\n",
    "    return string\n",
    "    \n",
    "dataset['image_path'] = dataset['image_path'].apply(remove_absolute)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>audio_id</th>\n",
       "      <th>hyps</th>\n",
       "      <th>text</th>\n",
       "      <th>speaker</th>\n",
       "      <th>audio_path</th>\n",
       "      <th>target</th>\n",
       "      <th>image_path</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Adam_Abramowicz_11_45_8-6</td>\n",
       "      <td>prog prezydenta powinien być przez państwa tyl...</td>\n",
       "      <td>krok prezydenta powinien być przez państwa tyl...</td>\n",
       "      <td>Adam_Abramowicz</td>\n",
       "      <td>data/sejm_voicelab/train/Adam_Abramowicz_11_45...</td>\n",
       "      <td>1</td>\n",
       "      <td>data/sejm_voicelab/train/Adam_Abramowicz_11_45...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Adam_Abramowicz_1_39_2-0</td>\n",
       "      <td>panie marszałku wysoki sejmie panie ministrze ...</td>\n",
       "      <td>panie marszałku wysoki sejmie panie ministrze ...</td>\n",
       "      <td>Adam_Abramowicz</td>\n",
       "      <td>data/sejm_voicelab/train/Adam_Abramowicz_1_39_...</td>\n",
       "      <td>1</td>\n",
       "      <td>data/sejm_voicelab/train/Adam_Abramowicz_1_39_...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AdamAbramowicz-20130410-file000</td>\n",
       "      <td>panie marszałku wysoka izbo panie ministrze pr...</td>\n",
       "      <td>panie marszałku wysoka izbo panie ministrze pr...</td>\n",
       "      <td>AdamAbramowicz</td>\n",
       "      <td>/home/raznem/projects/audio-pipelines/data/dat...</td>\n",
       "      <td>1</td>\n",
       "      <td>data/parlament_audio/SejmSenat/audio/AdamAbram...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AdamAbramowicz-20130410-file001</td>\n",
       "      <td>w grudniu kanada wystąpiła z protokółów z kiot...</td>\n",
       "      <td>w grudniu kanada wystąpiła z protokołu z kioto...</td>\n",
       "      <td>AdamAbramowicz</td>\n",
       "      <td>/home/raznem/projects/audio-pipelines/data/dat...</td>\n",
       "      <td>0</td>\n",
       "      <td>data/parlament_audio/SejmSenat/audio/AdamAbram...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>AdamAbramowicz-20130410-file002</td>\n",
       "      <td>panie marszałku wysoka izbo panie ministrze sw...</td>\n",
       "      <td>panie marszałku wysoka izbo panie ministrze w ...</td>\n",
       "      <td>AdamAbramowicz</td>\n",
       "      <td>/home/raznem/projects/audio-pipelines/data/dat...</td>\n",
       "      <td>0</td>\n",
       "      <td>data/parlament_audio/SejmSenat/audio/AdamAbram...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          audio_id  \\\n",
       "0        Adam_Abramowicz_11_45_8-6   \n",
       "1         Adam_Abramowicz_1_39_2-0   \n",
       "2  AdamAbramowicz-20130410-file000   \n",
       "3  AdamAbramowicz-20130410-file001   \n",
       "4  AdamAbramowicz-20130410-file002   \n",
       "\n",
       "                                                hyps  \\\n",
       "0  prog prezydenta powinien być przez państwa tyl...   \n",
       "1  panie marszałku wysoki sejmie panie ministrze ...   \n",
       "2  panie marszałku wysoka izbo panie ministrze pr...   \n",
       "3  w grudniu kanada wystąpiła z protokółów z kiot...   \n",
       "4  panie marszałku wysoka izbo panie ministrze sw...   \n",
       "\n",
       "                                                text          speaker  \\\n",
       "0  krok prezydenta powinien być przez państwa tyl...  Adam_Abramowicz   \n",
       "1  panie marszałku wysoki sejmie panie ministrze ...  Adam_Abramowicz   \n",
       "2  panie marszałku wysoka izbo panie ministrze pr...   AdamAbramowicz   \n",
       "3  w grudniu kanada wystąpiła z protokołu z kioto...   AdamAbramowicz   \n",
       "4  panie marszałku wysoka izbo panie ministrze w ...   AdamAbramowicz   \n",
       "\n",
       "                                          audio_path target  \\\n",
       "0  data/sejm_voicelab/train/Adam_Abramowicz_11_45...      1   \n",
       "1  data/sejm_voicelab/train/Adam_Abramowicz_1_39_...      1   \n",
       "2  /home/raznem/projects/audio-pipelines/data/dat...      1   \n",
       "3  /home/raznem/projects/audio-pipelines/data/dat...      0   \n",
       "4  /home/raznem/projects/audio-pipelines/data/dat...      0   \n",
       "\n",
       "                                          image_path  \n",
       "0  data/sejm_voicelab/train/Adam_Abramowicz_11_45...  \n",
       "1  data/sejm_voicelab/train/Adam_Abramowicz_1_39_...  \n",
       "2  data/parlament_audio/SejmSenat/audio/AdamAbram...  \n",
       "3  data/parlament_audio/SejmSenat/audio/AdamAbram...  \n",
       "4  data/parlament_audio/SejmSenat/audio/AdamAbram...  "
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/projects/audio-pipelines/data\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.to_csv('data/dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_path = dataset['image_path'][0]\n",
    "image = Image.open(image_path)\n",
    "image = image.convert('RGB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PIL.Image.Image"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/projects/audio-pipelines/data\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from allennlp.commands.elmo import ElmoEmbedder\n",
    "from torchvision import transforms\n",
    "\n",
    "try:\n",
    "    from PIL import Image\n",
    "except ImportError:\n",
    "    import Image\n",
    "    \n",
    "toImg = transforms.ToPILImage()\n",
    "toTensor = transforms.ToTensor()\n",
    "    \n",
    "class WavImagesLoader(Dataset):\n",
    "    def __init__(self, csv_path, transform=None):\n",
    "        self.dataset = pd.read_csv(csv_path)\n",
    "        self.transform = transform\n",
    "        \n",
    "    def __getitem__(self, key):\n",
    "        target = self.dataset['target'][key]\n",
    "        image_path = self.dataset['image_path'][key]\n",
    "        image = Image.open(image_path)\n",
    "        image = image.convert('RGB')\n",
    "        image = toTensor(image)\n",
    "        if self.transform is not None:\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        return image, target\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "    \n",
    "    @staticmethod        \n",
    "    def imshow(img):\n",
    "        img = img / 2 + 0.5     # unnormalize\n",
    "        npimg = img.numpy()\n",
    "        plt.imshow(np.transpose(npimg, (1, 2, 0)))    \n",
    "\n",
    "#     @staticmethod    \n",
    "#     def flaotTensorToImage(img, mean=0, std=1):\n",
    "#         \"\"\"convert a tensor to an image\"\"\"\n",
    "#         img = np.transpose(img.numpy(), (1, 2, 0))\n",
    "#         img = (img*std+ mean)*255\n",
    "#         img = img.astype(np.uint8)    \n",
    "#         return img    \n",
    "    \n",
    "    \n",
    "class ElmoWavImagesLoader(WavImagesLoader):\n",
    "    def __init__(self, csv_path, elmo_model, transform=None):\n",
    "        super().__init__(csv_path, transform=None)\n",
    "        self.elmo_model = elmo_model\n",
    "        \n",
    "    def __getitem__(self, key):\n",
    "        target = self.dataset['target'][key]\n",
    "        image_path = self.dataset['image_path'][key]\n",
    "        image = Image.open(path)\n",
    "        image = img.convert('RGB')\n",
    "        image = toTensor(image)\n",
    "        if self.transform is not None:\n",
    "            image = self.transform(image)\n",
    "            \n",
    "        text = self.dataset['hyps'][key]\n",
    "        text = text.split(' ')\n",
    "        embedding = self.elmo_model(text)\n",
    "        embedding = embedding.sum(axis=0)\n",
    "\n",
    "        return image, embedding, target\n",
    "    \n",
    "    \n",
    "class ElmoWavVecLoader(ElmoWavImagesLoader):\n",
    "    def __init__(self, csv_path, elmo_model, transform=None):\n",
    "        super().__init__(csv_path, elmo_model, transform=None)\n",
    "        \n",
    "    def __getitem__(self, key):\n",
    "        target = self.dataset['target'][key]\n",
    "        image_vec = self.dataset['image_vec'][key]\n",
    "        text = self.dataset['hyps'][key]\n",
    "        text = text.split(' ')\n",
    "        embedding = self.elmo_model(text)\n",
    "        embedding = embedding.sum(axis=0)\n",
    "\n",
    "        return image_vec, embedding, target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conv net to get vector from Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dropout = torch.nn.Dropout(p=0.30)\n",
    "class ConvRes(nn.Module):\n",
    "    def __init__(self, insize, outsize):\n",
    "        super(ConvRes, self).__init__()\n",
    "        drate = .3\n",
    "        self.math = nn.Sequential(\n",
    "            nn.BatchNorm2d(insize),\n",
    "            # nn.Dropout(drate),\n",
    "            torch.nn.Conv2d(insize, outsize, kernel_size=2, padding=2),\n",
    "            nn.PReLU(),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.math(x)\n",
    "\n",
    "\n",
    "class ConvCNN(nn.Module):\n",
    "    def __init__(self, insize, outsize, kernel_size=7, padding=2, pool=2, avg=True):\n",
    "        super(ConvCNN, self).__init__()\n",
    "        self.avg = avg\n",
    "        self.math = torch.nn.Sequential(\n",
    "            torch.nn.Conv2d(insize, outsize, kernel_size=kernel_size, padding=padding),\n",
    "            torch.nn.BatchNorm2d(outsize),\n",
    "            torch.nn.LeakyReLU(),\n",
    "            torch.nn.MaxPool2d(pool, pool),\n",
    "        )\n",
    "        self.avgpool = torch.nn.AvgPool2d(pool, pool)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.math(x)\n",
    "        if self.avg is True:\n",
    "            x = self.avgpool(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, output_dim):\n",
    "        super(Net, self).__init__()\n",
    "\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d(1)\n",
    "\n",
    "        self.cnn1 = ConvCNN(3, 32, kernel_size=7, pool=4, avg=False)\n",
    "        self.cnn2 = ConvCNN(32, 32, kernel_size=5, pool=2, avg=True)\n",
    "        self.cnn3 = ConvCNN(32, 32, kernel_size=5, pool=2, avg=True)\n",
    "\n",
    "        self.res1 = ConvRes(32, 64)\n",
    "\n",
    "        self.features = nn.Sequential(\n",
    "            self.cnn1, dropout,\n",
    "            self.cnn2,\n",
    "            self.cnn3,\n",
    "            self.res1,\n",
    "        )\n",
    "\n",
    "        self.classifier = torch.nn.Sequential(\n",
    "            nn.Linear(5184, output_dim),\n",
    "        )\n",
    "        self.sig = nn.Sigmoid()\n",
    "  \n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.classifier(x)\n",
    "        x = self.sig(x)\n",
    "        return x\n",
    "\n",
    "    \n",
    "# if use_cuda:\n",
    "#     lgr.info (\"Using the GPU\")\n",
    "#     model = Net(output_dim).cuda() # On GPU\n",
    "# else:\n",
    "#     lgr.info (\"Using the CPU\")\n",
    "#     model = Net(output_dim) # On CPU\n",
    "\n",
    "# lgr.info('Model {}'.format(model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "def image_trainer(loader_train, model, optimizer, exp_name='', save_every=None, print_every=None, epochs=1, use_gpu=True, \n",
    "                 dtype=torch.float32):\n",
    "    if use_gpu and torch.cuda.is_available():\n",
    "        device = torch.device('cuda:1')\n",
    "    else:\n",
    "        device = torch.device('cpu')\n",
    "        \n",
    "    model.train()\n",
    "    model.to(device=device)\n",
    "\n",
    "    for e in range(epochs):\n",
    "        print('Epoch %d' %e)\n",
    "        for t, (x, y) in enumerate(loader_train):\n",
    "            x = x.to(device=device, dtype=dtype)\n",
    "            y = y.to(device=device, dtype=dtype)\n",
    "\n",
    "            y_pred = model(x)\n",
    "            loss = loss_fn(y_pred, y)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "\n",
    "            optimizer.step()\n",
    "\n",
    "            if print_every is not None and t % print_every == 0:\n",
    "                print('Iteration %d, loss = %.4f; epoch %d' % (t, loss.item(), e))\n",
    "                \n",
    "        if save_every is not None and e % save_every == 0:\n",
    "            torch.save(model.state_dict(), f'models/{exp_name}_e%d_image_cnn.pt' % e)\n",
    "            gc.collect()\n",
    "    torch.save(model.state_dict(), f'models/{exp_name}_image_cnn.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = torch.nn.BCELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0\n",
      "Iteration 0, loss = 0.6898; epoch 0\n",
      "Iteration 100, loss = 0.6045; epoch 0\n",
      "Iteration 200, loss = 0.6556; epoch 0\n",
      "Iteration 300, loss = 0.6608; epoch 0\n",
      "Iteration 400, loss = 0.5873; epoch 0\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 64\n",
    "LEARNING_RATE = 1e-4\n",
    "USE_GPU = True\n",
    "TE_DTYPE = torch.float32\n",
    "TARGET_DIM = 1\n",
    "NUM_EPOCHS = 1\n",
    "\n",
    "model = Net(TARGET_DIM)\n",
    "image_data = WavImagesLoader('data/dataset.csv')\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "loader_train = DataLoader(image_data, batch_size=BATCH_SIZE, shuffle=True, num_workers=12)\n",
    "image_trainer(loader_train, model, optimizer, save_every=1, print_every=100, epochs=NUM_EPOCHS,\n",
    "      use_gpu=USE_GPU, dtype=TE_DTYPE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ls: cannot access '/home/raznem/projects/audio-pipelines/data/data/clarin_audio/audio/SES0088/': No such file or directory\n"
     ]
    }
   ],
   "source": [
    "!ls /home/raznem/projects/audio-pipelines/data/data/clarin_audio/audio/SES0088/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
