{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__Python VERSION: 3.7.5 (default, Oct 25 2019, 15:51:11) \n",
      "[GCC 7.3.0]\n",
      "__pyTorch VERSION: 1.2.0\n",
      "__CUDA VERSION\n",
      "/bin/sh: 1: nvcc: not found\n",
      "__CUDNN VERSION: 7301\n",
      "__Number CUDA Devices: 2\n",
      "__Devices\n",
      "Active CUDA Device: GPU 0\n",
      "Available devices  2\n",
      "Current cuda device  0\n",
      "3.7.5 (default, Oct 25 2019, 15:51:11) \n",
      "[GCC 7.3.0]\n",
      "13.5\n",
      "svmem(total=67477729280, available=44195655680, percent=34.5, used=22047756288, free=18291556352, active=19388362752, inactive=23464185856, buffers=97828864, cached=27040587776, shared=485748736, slab=5039849472)\n",
      "memory GB: 0.24345779418945312\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import sys\n",
    "import torch\n",
    "from torch.utils.data.dataset import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "\n",
    "from sklearn import model_selection\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import roc_auc_score, log_loss, roc_auc_score, roc_curve, auc\n",
    "from sklearn.model_selection import StratifiedKFold, ShuffleSplit, cross_val_score, train_test_split\n",
    "\n",
    "print('__Python VERSION:', sys.version)\n",
    "print('__pyTorch VERSION:', torch.__version__)\n",
    "print('__CUDA VERSION')\n",
    "from subprocess import call\n",
    "# call([\"nvcc\", \"--version\"]) does not work\n",
    "! nvcc --version\n",
    "print('__CUDNN VERSION:', torch.backends.cudnn.version())\n",
    "print('__Number CUDA Devices:', torch.cuda.device_count())\n",
    "print('__Devices')\n",
    "# call([\"nvidia-smi\", \"--format=csv\", \"--query-gpu=index,name,driver_version,memory.total,memory.used,memory.free\"])\n",
    "print('Active CUDA Device: GPU', torch.cuda.current_device())\n",
    "\n",
    "print ('Available devices ', torch.cuda.device_count())\n",
    "print ('Current cuda device ', torch.cuda.current_device())\n",
    "\n",
    "import numpy\n",
    "import numpy as np\n",
    "\n",
    "use_cuda = torch.cuda.is_available()\n",
    "FloatTensor = torch.cuda.FloatTensor if use_cuda else torch.FloatTensor\n",
    "LongTensor = torch.cuda.LongTensor if use_cuda else torch.LongTensor\n",
    "Tensor = FloatTensor\n",
    "\n",
    "import pandas\n",
    "import pandas as pd\n",
    "\n",
    "import logging\n",
    "handler=logging.basicConfig(level=logging.INFO)\n",
    "lgr = logging.getLogger(__name__)\n",
    "%matplotlib inline\n",
    "\n",
    "# !pip install psutil\n",
    "import psutil\n",
    "import os\n",
    "def cpuStats():\n",
    "        print(sys.version)\n",
    "        print(psutil.cpu_percent())\n",
    "        print(psutil.virtual_memory())  # physical memory usage\n",
    "        pid = os.getpid()\n",
    "        py = psutil.Process(pid)\n",
    "        memoryUse = py.memory_info()[0] / 2. ** 30  # memory use in GB...I think\n",
    "        print('memory GB:', memoryUse)\n",
    "\n",
    "cpuStats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_cuda = torch.cuda.is_available()\n",
    "# use_cuda = False\n",
    "\n",
    "FloatTensor = torch.cuda.FloatTensor if use_cuda else torch.FloatTensor\n",
    "LongTensor = torch.cuda.LongTensor if use_cuda else torch.LongTensor\n",
    "Tensor = FloatTensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/projects/audio-pipelines/data\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyps = pd.read_csv(\"sets/full_set/hyps\", names=['audio_id'])\n",
    "# hyps[['audio_id','hyps']] = hyps[\"audio_id\"].str.split(\" \", 1, expand=True)\n",
    "\n",
    "# targets = pd.read_csv(\"sets/full_set/targets\", names=['audio_id'])\n",
    "# targets[['audio_id','target']] = targets[\"audio_id\"].str.split(\" \", 1, expand=True)\n",
    "\n",
    "# text = pd.read_csv(\"sets/full_set/text\", names=['audio_id'])\n",
    "# text[['audio_id','text']] = text[\"audio_id\"].str.split(\" \", 1, expand=True)\n",
    "\n",
    "# speakers = pd.read_csv(\"sets/full_set/utt2spk\", names=['audio_id'])\n",
    "# speakers[['audio_id','speaker']] = speakers[\"audio_id\"].str.split(\" \", 1, expand=True)\n",
    "\n",
    "# images = pd.read_csv(\"sets/full_set/wav.scp\", names=['audio_id'])\n",
    "# images[['audio_id','audio_path']] = images[\"audio_id\"].str.split(\" \", 1, expand=True)\n",
    "\n",
    "# dataset = pd.merge(hyps, text, how=\"left\")\n",
    "# dataset = pd.merge(dataset, speakers, how=\"left\")\n",
    "# dataset = pd.merge(dataset, images, how=\"left\")\n",
    "# dataset = pd.merge(dataset, targets, how=\"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset['image_path'] = dataset['audio_path'].apply(lambda x : x[:-4] + \".png\")\n",
    "# def remove_absolute(string, prefix='/home/raznem/projects/audio-pipelines/data/'):\n",
    "#     if string.startswith(prefix):\n",
    "#         string = string[len(prefix):]\n",
    "#     return string\n",
    "    \n",
    "# dataset['image_path'] = dataset['image_path'].apply(remove_absolute)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv('data/dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "## from allennlp.commands.elmo import ElmoEmbedder\n",
    "from torchvision import transforms\n",
    "import numpy as np\n",
    "\n",
    "try:\n",
    "    from PIL import Image\n",
    "except ImportError:\n",
    "    import Image\n",
    "    \n",
    "toImg = transforms.ToPILImage()\n",
    "toTensor = transforms.ToTensor()\n",
    "    \n",
    "class WavImagesLoader(Dataset):\n",
    "    def __init__(self, csv_path, transform=None):\n",
    "        self.dataset = pd.read_csv(csv_path)\n",
    "        self.transform = transform\n",
    "        \n",
    "    def __getitem__(self, key):\n",
    "        target = self.dataset['target'][key]\n",
    "        image_path = self.dataset['image_path'][key]\n",
    "        image = Image.open(image_path)\n",
    "        image = image.convert('RGB')\n",
    "        image = toTensor(image)\n",
    "        if self.transform is not None:\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        return image, target\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "    \n",
    "    @staticmethod        \n",
    "    def imshow(img):\n",
    "        img = img / 2 + 0.5     # unnormalize\n",
    "        npimg = img.numpy()\n",
    "        plt.imshow(np.transpose(npimg, (1, 2, 0)))    \n",
    "\n",
    "#     @staticmethod    \n",
    "#     def flaotTensorToImage(img, mean=0, std=1):\n",
    "#         \"\"\"convert a tensor to an image\"\"\"\n",
    "#         img = np.transpose(img.numpy(), (1, 2, 0))\n",
    "#         img = (img*std+ mean)*255\n",
    "#         img = img.astype(np.uint8)    \n",
    "#         return img    \n",
    "    \n",
    "    \n",
    "class ElmoWavImagesLoader(WavImagesLoader):\n",
    "    def __init__(self, csv_path, text_vecs_path, transform=None):\n",
    "        super().__init__(csv_path, transform=None)\n",
    "        self.text_vecs_path = text_vecs_path\n",
    "        \n",
    "    def __getitem__(self, key):\n",
    "        target = self.dataset['target'][key]\n",
    "        image_path = self.dataset['image_path'][key]\n",
    "        image = Image.open(image_path)\n",
    "        image = image.convert('RGB')\n",
    "        image = toTensor(image)\n",
    "        if self.transform is not None:\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        text_vec = np.zeros((322, 1024))\n",
    "        tmp = np.load(self.text_vecs_path + self.dataset[\"audio_id\"][key] + \".npy\")\n",
    "        tmp = np.sum(tmp, axis=0)\n",
    "        text_len = tmp.shape[0]\n",
    "        text_vec[:tmp.shape[0],:tmp.shape[1]] = tmp\n",
    "        \n",
    "        return image, text_vec, text_len, target\n",
    "    \n",
    "    \n",
    "class ElmoWavVecLoader(ElmoWavImagesLoader):\n",
    "    def __init__(self, csv_path, text_vecs_path, wav_vecs_path, transform=None):\n",
    "        super().__init__(csv_path, text_vecs_path, transform=None)\n",
    "        self.wav_vecs_path = wav_vecs_path\n",
    "        \n",
    "    def __getitem__(self, key):\n",
    "        target = self.dataset['target'][key]\n",
    "        image_path = self.dataset['image_path'][key]\n",
    "        image_path = image_path[:-4] + \".np\"\n",
    "        wav_vec = np.load(self.wav_vecs_path + image_path, allow_pickle=True)\n",
    "\n",
    "        text_vec = np.zeros((322, 1024))\n",
    "        tmp = np.load(self.text_vecs_path + self.dataset[\"audio_id\"][key] + \".npy\")\n",
    "        tmp = np.sum(tmp, axis=0)\n",
    "        text_len = tmp.shape[0]\n",
    "        text_vec[:tmp.shape[0],:tmp.shape[1]] = tmp\n",
    "\n",
    "        return wav_vec, text_vec, text_len, target\n",
    "    \n",
    "class ElmoWavVecLoader_v2(WavImagesLoader):\n",
    "    def __init__(self, csv_path, text_vec_list, wav_vecs_path, transform=None):\n",
    "        super().__init__(csv_path, transform=None)\n",
    "        self.wav_vecs_path = wav_vecs_path\n",
    "        self.text_vec_list = text_vec_list\n",
    "        \n",
    "    def __getitem__(self, key):\n",
    "        target = self.dataset['target'][key]\n",
    "        image_path = self.dataset['image_path'][key]\n",
    "        image_path = image_path[:-4] + \".np\"\n",
    "        wav_vec = np.load(self.wav_vecs_path + image_path, allow_pickle=True)\n",
    "\n",
    "        text_vec = np.zeros((322, 1024))\n",
    "        tmp = self.text_vec_list[key]\n",
    "        text_len = tmp.shape[0]\n",
    "        text_vec[:tmp.shape[0],:tmp.shape[1]] = tmp\n",
    "\n",
    "        return wav_vec, text_vec, text_len, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 27731/27731 [00:17<00:00, 1616.09it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "files = os.listdir(\"../text_vecs/\")\n",
    "text_vec_list = []\n",
    "for i in tqdm(files):\n",
    "    if not i.endswith(\".npy\"):\n",
    "        continue\n",
    "    tmp = np.load(\"../text_vecs/\" + i)\n",
    "    tmp = np.sum(tmp, axis=0)\n",
    "    text_vec_list.append(tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset = pd.read_csv(\"data/dataset_dropna.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wav_vec_path = \"../wav2vec/\"\n",
    "# wav_vec_list = []\n",
    "# for i in tqdm(dataset[\"image_path\"]):\n",
    "#     image_path = i[:-4] + \".np\"\n",
    "#     wav_vec = np.load(wav_vec_path + image_path, allow_pickle=True)\n",
    "#     break\n",
    "#     wav_vec_list.append(wav_vec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get maximum length of text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# max_text_length = dataset_dropna['hyps'].apply(lambda x: len(x.split(' '))).max()\n",
    "# max_text_length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RNN to encode Elmo sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextWavModel(nn.Module):\n",
    "    def __init__(self, wav_dim, rnn_dim, layers_rnn=1, text_emb_dim=1024, wav_emb_dim=2115):\n",
    "        super().__init__()\n",
    "        self.layers_rnn = layers_rnn\n",
    "        self.text_emb_dim = text_emb_dim\n",
    "        self.rnn_dim = rnn_dim\n",
    "        self.wav_dim = wav_dim\n",
    "        self.wav_lstm = nn.LSTM(wav_emb_dim, self.wav_dim, num_layers=self.layers_rnn, batch_first=True)\n",
    "        self.text_lstm = nn.LSTM(text_emb_dim, self.rnn_dim, num_layers=self.layers_rnn, batch_first=True)\n",
    "        self.fc_last = nn.Linear(self.wav_dim + self.rnn_dim, 1)\n",
    "        self.sig = nn.Sigmoid()\n",
    "        \n",
    "    def forward(self, wav, text, text_len):\n",
    "        wav = torch.squeeze(wav)\n",
    "        _, (wav_out, _) = self.wav_lstm(wav)\n",
    "        wav_out = wav_out.reshape(-1, self.rnn_dim)\n",
    "        \n",
    "        seq_lengths = text_len\n",
    "        pack = torch.nn.utils.rnn.pack_padded_sequence(text, seq_lengths, batch_first=True)\n",
    "        _, (rnn_out, _) = self.text_lstm(text)\n",
    "        rnn_out = rnn_out.reshape(-1, self.rnn_dim)\n",
    "        \n",
    "        x = torch.cat([wav_out, rnn_out], dim=1)\n",
    "        y_pred = self.fc_last(x)\n",
    "        y_pred = torch.squeeze(y_pred)\n",
    "        y_pred = self.sig(y_pred)\n",
    "        return y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "def wavtext_trainer(loader_train, model, optimizer, exp_name='', save_every=None, print_every=None, epochs=1, use_gpu=True, \n",
    "                 dtype=torch.float32):\n",
    "    if use_gpu and torch.cuda.is_available():\n",
    "        device = torch.device('cuda:0')\n",
    "    else:\n",
    "        device = torch.device('cpu')\n",
    "        \n",
    "    model.train()\n",
    "    model.to(device=device)\n",
    "\n",
    "    for e in range(epochs):\n",
    "        print('Epoch %d' %e)\n",
    "        acc = 0\n",
    "        files_no = 0\n",
    "        for t, (wav, text, text_len, y) in enumerate(loader_train):\n",
    "            wav = wav.to(device=device, dtype=dtype)\n",
    "            text = text.to(device=device, dtype=dtype)\n",
    "            y = y.to(device=device, dtype=dtype)\n",
    "            text_len, perm_idx = text_len.sort(0, descending=True)\n",
    "            text = text[perm_idx]\n",
    "            wav = wav[perm_idx]\n",
    "            y = y[perm_idx]\n",
    "            \n",
    "            y_pred = model(wav, text, text_len)\n",
    "            \n",
    "            loss = loss_fn(y_pred, y)\n",
    "            \n",
    "            acc += (torch.round(y_pred.cpu()) == y.cpu()).sum().type(torch.float32)\n",
    "            files_no += len(text)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "\n",
    "            optimizer.step()\n",
    "\n",
    "            if print_every is not None and t % print_every == 0:\n",
    "                print('Iteration %d, loss = %.4f; epoch %d' % (t, loss.item(), e))\n",
    "                print(f'Acc {acc / files_no}')\n",
    "                \n",
    "                \n",
    "        if save_every is not None and e % save_every == 0:\n",
    "            torch.save(model.state_dict(), f'models/{exp_name}_e%d_cnn_rnn.pt' % e)\n",
    "            gc.collect()\n",
    "    torch.save(model.state_dict(), f'models/{exp_name}_cnn_rnn.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0\n",
      "Iteration 0, loss = 0.6917; epoch 0\n",
      "Acc 0.546875\n",
      "Iteration 100, loss = 0.6423; epoch 0\n",
      "Acc 0.6050432920455933\n",
      "Iteration 200, loss = 0.6622; epoch 0\n",
      "Acc 0.6050994992256165\n",
      "Iteration 300, loss = 0.6831; epoch 0\n",
      "Acc 0.6066237688064575\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 64\n",
    "LEARNING_RATE = 1e-4\n",
    "USE_GPU = True\n",
    "TE_DTYPE = torch.float32\n",
    "WAV_DIM = 128\n",
    "RNN_DIM = 128\n",
    "NUM_EPOCHS = 100\n",
    "\n",
    "loss_fn = torch.nn.BCELoss()\n",
    "model = TextWavModel(WAV_DIM, RNN_DIM)\n",
    "image_data = ElmoWavVecLoader_v2('data/dataset_dropna.csv', text_vec_list, '../wav2vec/')\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "loader_train = DataLoader(image_data, batch_size=BATCH_SIZE, shuffle=True, num_workers=12)\n",
    "wavtext_trainer(loader_train, model, optimizer, exp_name='', save_every=10, print_every=100, epochs=NUM_EPOCHS,\n",
    "      use_gpu=USE_GPU, dtype=TE_DTYPE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
