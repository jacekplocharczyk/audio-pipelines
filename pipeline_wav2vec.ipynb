{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__Python VERSION: 3.7.5 (default, Oct 25 2019, 15:51:11) \n",
      "[GCC 7.3.0]\n",
      "__pyTorch VERSION: 1.2.0\n",
      "__CUDA VERSION\n",
      "/bin/sh: 1: nvcc: not found\n",
      "__CUDNN VERSION: 7301\n",
      "__Number CUDA Devices: 2\n",
      "__Devices\n",
      "Active CUDA Device: GPU 0\n",
      "Available devices  2\n",
      "Current cuda device  0\n",
      "3.7.5 (default, Oct 25 2019, 15:51:11) \n",
      "[GCC 7.3.0]\n",
      "13.5\n",
      "svmem(total=67477729280, available=44195655680, percent=34.5, used=22047756288, free=18291556352, active=19388362752, inactive=23464185856, buffers=97828864, cached=27040587776, shared=485748736, slab=5039849472)\n",
      "memory GB: 0.24345779418945312\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import sys\n",
    "import torch\n",
    "from torch.utils.data.dataset import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "\n",
    "from sklearn import model_selection\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import roc_auc_score, log_loss, roc_auc_score, roc_curve, auc\n",
    "from sklearn.model_selection import StratifiedKFold, ShuffleSplit, cross_val_score, train_test_split\n",
    "\n",
    "print('__Python VERSION:', sys.version)\n",
    "print('__pyTorch VERSION:', torch.__version__)\n",
    "print('__CUDA VERSION')\n",
    "from subprocess import call\n",
    "# call([\"nvcc\", \"--version\"]) does not work\n",
    "! nvcc --version\n",
    "print('__CUDNN VERSION:', torch.backends.cudnn.version())\n",
    "print('__Number CUDA Devices:', torch.cuda.device_count())\n",
    "print('__Devices')\n",
    "# call([\"nvidia-smi\", \"--format=csv\", \"--query-gpu=index,name,driver_version,memory.total,memory.used,memory.free\"])\n",
    "print('Active CUDA Device: GPU', torch.cuda.current_device())\n",
    "\n",
    "print ('Available devices ', torch.cuda.device_count())\n",
    "print ('Current cuda device ', torch.cuda.current_device())\n",
    "\n",
    "import numpy\n",
    "import numpy as np\n",
    "\n",
    "use_cuda = torch.cuda.is_available()\n",
    "FloatTensor = torch.cuda.FloatTensor if use_cuda else torch.FloatTensor\n",
    "LongTensor = torch.cuda.LongTensor if use_cuda else torch.LongTensor\n",
    "Tensor = FloatTensor\n",
    "\n",
    "import pandas\n",
    "import pandas as pd\n",
    "\n",
    "import logging\n",
    "handler=logging.basicConfig(level=logging.INFO)\n",
    "lgr = logging.getLogger(__name__)\n",
    "%matplotlib inline\n",
    "\n",
    "# !pip install psutil\n",
    "import psutil\n",
    "import os\n",
    "def cpuStats():\n",
    "        print(sys.version)\n",
    "        print(psutil.cpu_percent())\n",
    "        print(psutil.virtual_memory())  # physical memory usage\n",
    "        pid = os.getpid()\n",
    "        py = psutil.Process(pid)\n",
    "        memoryUse = py.memory_info()[0] / 2. ** 30  # memory use in GB...I think\n",
    "        print('memory GB:', memoryUse)\n",
    "\n",
    "cpuStats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_cuda = torch.cuda.is_available()\n",
    "# use_cuda = False\n",
    "\n",
    "FloatTensor = torch.cuda.FloatTensor if use_cuda else torch.FloatTensor\n",
    "LongTensor = torch.cuda.LongTensor if use_cuda else torch.LongTensor\n",
    "Tensor = FloatTensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/projects/audio-pipelines/data\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyps = pd.read_csv(\"sets/full_set/hyps\", names=['audio_id'])\n",
    "# hyps[['audio_id','hyps']] = hyps[\"audio_id\"].str.split(\" \", 1, expand=True)\n",
    "\n",
    "# targets = pd.read_csv(\"sets/full_set/targets\", names=['audio_id'])\n",
    "# targets[['audio_id','target']] = targets[\"audio_id\"].str.split(\" \", 1, expand=True)\n",
    "\n",
    "# text = pd.read_csv(\"sets/full_set/text\", names=['audio_id'])\n",
    "# text[['audio_id','text']] = text[\"audio_id\"].str.split(\" \", 1, expand=True)\n",
    "\n",
    "# speakers = pd.read_csv(\"sets/full_set/utt2spk\", names=['audio_id'])\n",
    "# speakers[['audio_id','speaker']] = speakers[\"audio_id\"].str.split(\" \", 1, expand=True)\n",
    "\n",
    "# images = pd.read_csv(\"sets/full_set/wav.scp\", names=['audio_id'])\n",
    "# images[['audio_id','audio_path']] = images[\"audio_id\"].str.split(\" \", 1, expand=True)\n",
    "\n",
    "# dataset = pd.merge(hyps, text, how=\"left\")\n",
    "# dataset = pd.merge(dataset, speakers, how=\"left\")\n",
    "# dataset = pd.merge(dataset, images, how=\"left\")\n",
    "# dataset = pd.merge(dataset, targets, how=\"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset['image_path'] = dataset['audio_path'].apply(lambda x : x[:-4] + \".png\")\n",
    "# def remove_absolute(string, prefix='/home/raznem/projects/audio-pipelines/data/'):\n",
    "#     if string.startswith(prefix):\n",
    "#         string = string[len(prefix):]\n",
    "#     return string\n",
    "    \n",
    "# dataset['image_path'] = dataset['image_path'].apply(remove_absolute)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv('data/dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "## from allennlp.commands.elmo import ElmoEmbedder\n",
    "from torchvision import transforms\n",
    "import numpy as np\n",
    "\n",
    "try:\n",
    "    from PIL import Image\n",
    "except ImportError:\n",
    "    import Image\n",
    "    \n",
    "toImg = transforms.ToPILImage()\n",
    "toTensor = transforms.ToTensor()\n",
    "    \n",
    "class WavImagesLoader(Dataset):\n",
    "    def __init__(self, csv_path, transform=None):\n",
    "        self.dataset = pd.read_csv(csv_path)\n",
    "        self.transform = transform\n",
    "        \n",
    "    def __getitem__(self, key):\n",
    "        target = self.dataset['target'][key]\n",
    "        image_path = self.dataset['image_path'][key]\n",
    "        image = Image.open(image_path)\n",
    "        image = image.convert('RGB')\n",
    "        image = toTensor(image)\n",
    "        if self.transform is not None:\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        return image, target\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "    \n",
    "    @staticmethod        \n",
    "    def imshow(img):\n",
    "        img = img / 2 + 0.5     # unnormalize\n",
    "        npimg = img.numpy()\n",
    "        plt.imshow(np.transpose(npimg, (1, 2, 0)))    \n",
    "\n",
    "#     @staticmethod    \n",
    "#     def flaotTensorToImage(img, mean=0, std=1):\n",
    "#         \"\"\"convert a tensor to an image\"\"\"\n",
    "#         img = np.transpose(img.numpy(), (1, 2, 0))\n",
    "#         img = (img*std+ mean)*255\n",
    "#         img = img.astype(np.uint8)    \n",
    "#         return img    \n",
    "    \n",
    "    \n",
    "class ElmoWavImagesLoader(WavImagesLoader):\n",
    "    def __init__(self, csv_path, text_vecs_path, transform=None):\n",
    "        super().__init__(csv_path, transform=None)\n",
    "        self.text_vecs_path = text_vecs_path\n",
    "        \n",
    "    def __getitem__(self, key):\n",
    "        target = self.dataset['target'][key]\n",
    "        image_path = self.dataset['image_path'][key]\n",
    "        image = Image.open(image_path)\n",
    "        image = image.convert('RGB')\n",
    "        image = toTensor(image)\n",
    "        if self.transform is not None:\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        text_vec = np.zeros((322, 1024))\n",
    "        tmp = np.load(self.text_vecs_path + self.dataset[\"audio_id\"][key] + \".npy\")\n",
    "        tmp = np.sum(tmp, axis=0)\n",
    "        text_len = tmp.shape[0]\n",
    "        text_vec[:tmp.shape[0],:tmp.shape[1]] = tmp\n",
    "        \n",
    "        return image, text_vec, text_len, target\n",
    "    \n",
    "    \n",
    "class ElmoWavVecLoader(ElmoWavImagesLoader):\n",
    "    def __init__(self, csv_path, text_vecs_path, wav_vecs_path, transform=None):\n",
    "        super().__init__(csv_path, text_vecs_path, transform=None)\n",
    "        self.wav_vecs_path = wav_vecs_path\n",
    "        \n",
    "    def __getitem__(self, key):\n",
    "        target = self.dataset['target'][key]\n",
    "        image_path = self.dataset['image_path'][key]\n",
    "        image_path = image_path[:-4] + \".np\"\n",
    "        wav_vec = np.load(self.wav_vecs_path + image_path, allow_pickle=True)\n",
    "\n",
    "        text_vec = np.zeros((322, 1024))\n",
    "        tmp = np.load(self.text_vecs_path + self.dataset[\"audio_id\"][key] + \".npy\")\n",
    "        tmp = np.sum(tmp, axis=0)\n",
    "        text_len = tmp.shape[0]\n",
    "        text_vec[:tmp.shape[0],:tmp.shape[1]] = tmp\n",
    "\n",
    "        return wav_vec, text_vec, text_len, target\n",
    "    \n",
    "class ElmoWavVecLoader_v2(WavImagesLoader):\n",
    "    def __init__(self, csv_path, text_vec_list, wav_vecs_path, transform=None):\n",
    "        super().__init__(csv_path, transform=None)\n",
    "        self.wav_vecs_path = wav_vecs_path\n",
    "        self.text_vec_list = text_vec_list\n",
    "        \n",
    "    def __getitem__(self, key):\n",
    "        target = self.dataset['target'][key]\n",
    "        image_path = self.dataset['image_path'][key]\n",
    "        image_path = image_path[:-4] + \".np\"\n",
    "        wav_vec = np.load(self.wav_vecs_path + image_path, allow_pickle=True)\n",
    "\n",
    "        text_vec = np.zeros((322, 1024))\n",
    "        tmp = self.text_vec_list[key]\n",
    "        text_len = tmp.shape[0]\n",
    "        text_vec[:tmp.shape[0],:tmp.shape[1]] = tmp\n",
    "\n",
    "        return wav_vec, text_vec, text_len, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 27731/27731 [00:17<00:00, 1616.09it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "files = os.listdir(\"../text_vecs/\")\n",
    "text_vec_list = []\n",
    "for i in tqdm(files):\n",
    "    if not i.endswith(\".npy\"):\n",
    "        continue\n",
    "    tmp = np.load(\"../text_vecs/\" + i)\n",
    "    tmp = np.sum(tmp, axis=0)\n",
    "    text_vec_list.append(tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset = pd.read_csv(\"data/dataset_dropna.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wav_vec_path = \"../wav2vec/\"\n",
    "# wav_vec_list = []\n",
    "# for i in tqdm(dataset[\"image_path\"]):\n",
    "#     image_path = i[:-4] + \".np\"\n",
    "#     wav_vec = np.load(wav_vec_path + image_path, allow_pickle=True)\n",
    "#     break\n",
    "#     wav_vec_list.append(wav_vec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get maximum length of text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# max_text_length = dataset_dropna['hyps'].apply(lambda x: len(x.split(' '))).max()\n",
    "# max_text_length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RNN to encode Elmo sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextWavModel(nn.Module):\n",
    "    def __init__(self, wav_dim, rnn_dim, layers_rnn=1, text_emb_dim=1024, wav_emb_dim=2115):\n",
    "        super().__init__()\n",
    "        self.layers_rnn = layers_rnn\n",
    "        self.text_emb_dim = text_emb_dim\n",
    "        self.rnn_dim = rnn_dim\n",
    "        self.wav_dim = wav_dim\n",
    "        # Check if weights of both LSTMs are changing\n",
    "        self.wav_lstm = nn.LSTM(wav_emb_dim, self.wav_dim, num_layers=self.layers_rnn, batch_first=True) \n",
    "        self.text_lstm = nn.LSTM(text_emb_dim, self.rnn_dim, num_layers=self.layers_rnn, batch_first=True)\n",
    "        self.fc_last = nn.Linear(self.wav_dim + self.rnn_dim, 1)\n",
    "        self.sig = nn.Sigmoid()\n",
    "        \n",
    "    def forward(self, wav, text, text_len):\n",
    "        wav = torch.squeeze(wav)\n",
    "        _, (wav_out, _) = self.wav_lstm(wav)\n",
    "        wav_out = wav_out.reshape(-1, self.wav_dim) # check if all dims are ok\n",
    "        \n",
    "        seq_lengths = text_len\n",
    "        pack = torch.nn.utils.rnn.pack_padded_sequence(text, seq_lengths, batch_first=True) # check pack_padded_sequence output\n",
    "        _, (rnn_out, _) = self.text_lstm(text)\n",
    "        rnn_out = rnn_out.reshape(-1, self.rnn_dim) # check if all dims are ok\n",
    "        \n",
    "        x = torch.cat([wav_out, rnn_out], dim=1)\n",
    "        y_pred = self.fc_last(x)\n",
    "        y_pred = torch.squeeze(y_pred)\n",
    "        y_pred = self.sig(y_pred)\n",
    "        return y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "def wavtext_trainer(loader_train, model, optimizer, exp_name='', save_every=None, print_every=None, epochs=1, use_gpu=True, \n",
    "                 dtype=torch.float32):\n",
    "    if use_gpu and torch.cuda.is_available():\n",
    "        device = torch.device('cuda:0')\n",
    "    else:\n",
    "        device = torch.device('cpu')\n",
    "        \n",
    "    model.train()\n",
    "    model.to(device=device)\n",
    "\n",
    "    for e in range(epochs):\n",
    "        print('Epoch %d' %e)\n",
    "        acc = 0\n",
    "        files_no = 0\n",
    "        for t, (wav, text, text_len, y) in enumerate(loader_train):\n",
    "            wav = wav.to(device=device, dtype=dtype)\n",
    "            text = text.to(device=device, dtype=dtype)\n",
    "            y = y.to(device=device, dtype=dtype)\n",
    "            text_len, perm_idx = text_len.sort(0, descending=True) # check how sort work and it's output\n",
    "            text = text[perm_idx]\n",
    "            wav = wav[perm_idx]\n",
    "            y = y[perm_idx]\n",
    "            \n",
    "            y_pred = model(wav, text, text_len)\n",
    "            \n",
    "            loss = loss_fn(y_pred, y)\n",
    "            \n",
    "            acc += (torch.round(y_pred.cpu()) == y.cpu()).sum().type(torch.float32)\n",
    "            files_no += len(text)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "\n",
    "            optimizer.step()\n",
    "\n",
    "            if print_every is not None and t % print_every == 0:\n",
    "                print('Iteration %d, loss = %.4f; epoch %d' % (t, loss.item(), e))\n",
    "                print(f'Acc {acc / files_no}')\n",
    "                \n",
    "                \n",
    "        if save_every is not None and e % save_every == 0:\n",
    "            torch.save(model.state_dict(), f'models/{exp_name}_e%d_cnn_rnn.pt' % e)\n",
    "            gc.collect()\n",
    "    torch.save(model.state_dict(), f'models/{exp_name}_cnn_rnn.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0\n",
      "Iteration 0, loss = 0.6917; epoch 0\n",
      "Acc 0.546875\n",
      "Iteration 100, loss = 0.6423; epoch 0\n",
      "Acc 0.6050432920455933\n",
      "Iteration 200, loss = 0.6622; epoch 0\n",
      "Acc 0.6050994992256165\n",
      "Iteration 300, loss = 0.6831; epoch 0\n",
      "Acc 0.6066237688064575\n",
      "Iteration 400, loss = 0.6177; epoch 0\n",
      "Acc 0.6077384948730469\n",
      "Epoch 1\n",
      "Iteration 0, loss = 0.6545; epoch 1\n",
      "Acc 0.640625\n",
      "Iteration 100, loss = 0.6508; epoch 1\n",
      "Acc 0.6047338843345642\n",
      "Iteration 200, loss = 0.6691; epoch 1\n",
      "Acc 0.6074315905570984\n",
      "Iteration 300, loss = 0.6620; epoch 1\n",
      "Acc 0.6083887219429016\n",
      "Iteration 400, loss = 0.6759; epoch 1\n",
      "Acc 0.6088294982910156\n",
      "Epoch 2\n",
      "Iteration 0, loss = 0.6625; epoch 2\n",
      "Acc 0.625\n",
      "Iteration 100, loss = 0.6924; epoch 2\n",
      "Acc 0.6141707897186279\n",
      "Iteration 200, loss = 0.6360; epoch 2\n",
      "Acc 0.6101523637771606\n",
      "Iteration 300, loss = 0.7025; epoch 2\n",
      "Acc 0.607765793800354\n",
      "Iteration 400, loss = 0.6691; epoch 2\n",
      "Acc 0.6080501675605774\n",
      "Epoch 3\n",
      "Iteration 0, loss = 0.6555; epoch 3\n",
      "Acc 0.640625\n",
      "Iteration 100, loss = 0.6756; epoch 3\n",
      "Acc 0.6031869053840637\n",
      "Iteration 200, loss = 0.6476; epoch 3\n",
      "Acc 0.6065765023231506\n",
      "Iteration 300, loss = 0.6757; epoch 3\n",
      "Acc 0.6055855751037598\n",
      "Iteration 400, loss = 0.6690; epoch 3\n",
      "Acc 0.6073487997055054\n",
      "Epoch 4\n",
      "Iteration 0, loss = 0.6548; epoch 4\n",
      "Acc 0.640625\n",
      "Iteration 100, loss = 0.7204; epoch 4\n",
      "Acc 0.6133973002433777\n",
      "Iteration 200, loss = 0.6880; epoch 4\n",
      "Acc 0.6047885417938232\n",
      "Iteration 300, loss = 0.6560; epoch 4\n",
      "Acc 0.6051702499389648\n",
      "Iteration 400, loss = 0.6763; epoch 4\n",
      "Acc 0.6074267625808716\n",
      "Epoch 5\n",
      "Iteration 0, loss = 0.6263; epoch 5\n",
      "Acc 0.703125\n",
      "Iteration 100, loss = 0.6459; epoch 5\n",
      "Acc 0.6239171028137207\n",
      "Iteration 200, loss = 0.7089; epoch 5\n",
      "Acc 0.614116907119751\n",
      "Iteration 300, loss = 0.6327; epoch 5\n",
      "Acc 0.6086482405662537\n",
      "Iteration 400, loss = 0.7166; epoch 5\n",
      "Acc 0.6080501675605774\n",
      "Epoch 6\n",
      "Iteration 0, loss = 0.6948; epoch 6\n",
      "Acc 0.546875\n",
      "Iteration 100, loss = 0.7175; epoch 6\n",
      "Acc 0.6116955280303955\n",
      "Iteration 200, loss = 0.7157; epoch 6\n",
      "Acc 0.6053327322006226\n",
      "Iteration 300, loss = 0.6622; epoch 6\n",
      "Acc 0.6085444092750549\n",
      "Iteration 400, loss = 0.6628; epoch 6\n",
      "Acc 0.6080501675605774\n",
      "Epoch 7\n",
      "Iteration 0, loss = 0.6627; epoch 7\n",
      "Acc 0.625\n",
      "Iteration 100, loss = 0.6916; epoch 7\n",
      "Acc 0.6120049357414246\n",
      "Iteration 200, loss = 0.6761; epoch 7\n",
      "Acc 0.6099969148635864\n",
      "Iteration 300, loss = 0.6520; epoch 7\n",
      "Acc 0.6069352030754089\n",
      "Iteration 400, loss = 0.6757; epoch 7\n",
      "Acc 0.6069981455802917\n",
      "Epoch 8\n",
      "Iteration 0, loss = 0.6546; epoch 8\n",
      "Acc 0.640625\n",
      "Iteration 100, loss = 0.6757; epoch 8\n",
      "Acc 0.6103032231330872\n",
      "Iteration 200, loss = 0.6621; epoch 8\n",
      "Acc 0.609375\n",
      "Iteration 300, loss = 0.7011; epoch 8\n",
      "Acc 0.607090950012207\n",
      "Iteration 400, loss = 0.6772; epoch 8\n",
      "Acc 0.6086346507072449\n",
      "Epoch 9\n",
      "Iteration 0, loss = 0.6690; epoch 9\n",
      "Acc 0.609375\n",
      "Iteration 100, loss = 0.6759; epoch 9\n",
      "Acc 0.6120049357414246\n",
      "Iteration 200, loss = 0.6406; epoch 9\n",
      "Acc 0.6120180487632751\n",
      "Iteration 300, loss = 0.6880; epoch 9\n",
      "Acc 0.6082329750061035\n",
      "Iteration 400, loss = 0.7123; epoch 9\n",
      "Acc 0.6091801524162292\n",
      "Epoch 10\n",
      "Iteration 0, loss = 0.7022; epoch 10\n",
      "Acc 0.53125\n",
      "Iteration 100, loss = 0.6901; epoch 10\n",
      "Acc 0.6073638796806335\n",
      "Iteration 200, loss = 0.6961; epoch 10\n",
      "Acc 0.6070429086685181\n",
      "Iteration 300, loss = 0.6756; epoch 10\n",
      "Acc 0.6052221655845642\n",
      "Iteration 400, loss = 0.6729; epoch 10\n",
      "Acc 0.6070760488510132\n",
      "Epoch 11\n",
      "Iteration 0, loss = 0.6396; epoch 11\n",
      "Acc 0.671875\n",
      "Iteration 100, loss = 0.6477; epoch 11\n",
      "Acc 0.6154084205627441\n",
      "Iteration 200, loss = 0.7032; epoch 11\n",
      "Acc 0.6133395433425903\n",
      "Iteration 300, loss = 0.7178; epoch 11\n",
      "Acc 0.606260359287262\n",
      "Iteration 400, loss = 0.5958; epoch 11\n",
      "Acc 0.6076605319976807\n",
      "Epoch 12\n",
      "Iteration 0, loss = 0.6691; epoch 12\n",
      "Acc 0.609375\n",
      "Iteration 100, loss = 0.7112; epoch 12\n",
      "Acc 0.6086015105247498\n",
      "Iteration 200, loss = 0.6438; epoch 12\n",
      "Acc 0.6030783653259277\n",
      "Iteration 300, loss = 0.7035; epoch 12\n",
      "Acc 0.603976309299469\n",
      "Iteration 400, loss = 0.6470; epoch 12\n",
      "Acc 0.6075436472892761\n",
      "Epoch 13\n",
      "Iteration 0, loss = 0.6842; epoch 13\n",
      "Acc 0.578125\n",
      "Iteration 100, loss = 0.6690; epoch 13\n",
      "Acc 0.6064356565475464\n",
      "Iteration 200, loss = 0.6821; epoch 13\n",
      "Acc 0.6053327322006226\n",
      "Iteration 300, loss = 0.6336; epoch 13\n",
      "Acc 0.6078177094459534\n",
      "Iteration 400, loss = 0.6622; epoch 13\n",
      "Acc 0.6080112457275391\n",
      "Epoch 14\n",
      "Iteration 0, loss = 0.6622; epoch 14\n",
      "Acc 0.625\n",
      "Iteration 100, loss = 0.6907; epoch 14\n",
      "Acc 0.6146348714828491\n",
      "Iteration 200, loss = 0.6547; epoch 14\n",
      "Acc 0.6143501400947571\n",
      "Iteration 300, loss = 0.6634; epoch 14\n",
      "Acc 0.6085963249206543\n",
      "Iteration 400, loss = 0.6562; epoch 14\n",
      "Acc 0.6079332828521729\n",
      "Epoch 15\n",
      "Iteration 0, loss = 0.6413; epoch 15\n",
      "Acc 0.671875\n",
      "Iteration 100, loss = 0.6692; epoch 15\n",
      "Acc 0.619585394859314\n",
      "Iteration 200, loss = 0.6627; epoch 15\n",
      "Acc 0.6111629605293274\n",
      "Iteration 300, loss = 0.6834; epoch 15\n",
      "Acc 0.6106208562850952\n",
      "Iteration 400, loss = 0.6690; epoch 15\n",
      "Acc 0.6091801524162292\n",
      "Epoch 16\n",
      "Iteration 0, loss = 0.6557; epoch 16\n",
      "Acc 0.640625\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-1990d30432c3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0mloader_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_workers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m12\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m wavtext_trainer(loader_train, model, optimizer, exp_name='', save_every=None, print_every=100, epochs=NUM_EPOCHS,\n\u001b[0;32m---> 15\u001b[0;31m       use_gpu=USE_GPU, dtype=TE_DTYPE)\n\u001b[0m",
      "\u001b[0;32m<ipython-input-15-dd7eba16f9a8>\u001b[0m in \u001b[0;36mwavtext_trainer\u001b[0;34m(loader_train, model, optimizer, exp_name, save_every, print_every, epochs, use_gpu, dtype)\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0mfiles_no\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mwav\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext_len\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloader_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m             \u001b[0mwav\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwav\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m             \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m             \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 64\n",
    "LEARNING_RATE = 1e-4\n",
    "USE_GPU = True\n",
    "TE_DTYPE = torch.float32\n",
    "WAV_DIM = 128\n",
    "RNN_DIM = 128\n",
    "NUM_EPOCHS = 100\n",
    "\n",
    "loss_fn = torch.nn.BCELoss()\n",
    "model = TextWavModel(WAV_DIM, RNN_DIM)\n",
    "image_data = ElmoWavVecLoader_v2('data/dataset_dropna.csv', text_vec_list, '../wav2vec/')\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "loader_train = DataLoader(image_data, batch_size=BATCH_SIZE, shuffle=True, num_workers=12)\n",
    "wavtext_trainer(loader_train, model, optimizer, exp_name='', save_every=10, print_every=100, epochs=NUM_EPOCHS,\n",
    "      use_gpu=USE_GPU, dtype=TE_DTYPE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
