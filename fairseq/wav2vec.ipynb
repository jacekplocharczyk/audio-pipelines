{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/projects/aigames_data\n"
     ]
    }
   ],
   "source": [
    "%cd /home/jupyter/audio-pipelines/fairseq/data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from fairseq.models.wav2vec import Wav2VecModel\n",
    "import torchaudio\n",
    "\n",
    "from typing import List\n",
    "\n",
    "WAV_SCP_PATH = \"/home/jupyter/audio-pipelines/fairseq/dummy_wav.scp\"\n",
    "MAX_PADDING = 2032307 // 6\n",
    "\n",
    "WAV2VEC_PATH = \"/home/jupyter/audio-pipelines/fairseq/wav2vec_large.pt\"\n",
    "DEVICE = \"cuda:0\"\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "\n",
    "def convert_wave(wave: torch.tensor, model: torch.nn.Module):\n",
    "    with torch.no_grad():\n",
    "        z = model.feature_extractor(wave.to(DEVICE))\n",
    "        c = model.feature_aggregator(z)\n",
    "        del z\n",
    "    return c.cpu()\n",
    "\n",
    "\n",
    "def load_model() -> torch.nn.Module:\n",
    "    cp = torch.load(WAV2VEC_PATH)\n",
    "    model = Wav2VecModel.build_model(cp[\"args\"], task=None)\n",
    "    model.load_state_dict(cp[\"model\"])\n",
    "    model = model.eval()\n",
    "    return model.to(DEVICE)\n",
    "\n",
    "\n",
    "def save_file(tensor: torch.tensor, output_path: str):\n",
    "    torch.save(tensor, output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from typing import Tuple\n",
    "import torch\n",
    "from torch import nn\n",
    "import math\n",
    "\n",
    "class WavDataset(Dataset):\n",
    "    \"\"\"Face Landmarks dataset.\"\"\"\n",
    "\n",
    "    def __init__(self, scp_path: str):\n",
    "        self.scp_path = scp_path\n",
    "        self.wav_paths = self.extract_wav_paths()\n",
    "\n",
    "    def extract_wav_paths(self):\n",
    "        paths = []\n",
    "        with open(self.scp_path, \"r\") as f:\n",
    "            for row in f:\n",
    "                wav_path = row.split(\" \")[1].split(\"\\n\")[0]\n",
    "                paths.append(wav_path)\n",
    "        return paths\n",
    "\n",
    "    def padding(self, wave: torch.tensor) -> Tuple[torch.tensor, int]:\n",
    "        wave = wave[0, :MAX_PADDING]\n",
    "        len_ = wave.shape[0]\n",
    "        orginal_len = MAX_PADDING - len_\n",
    "\n",
    "        padded = nn.ConstantPad1d((0, orginal_len), 0)(wave)\n",
    "        return padded, orginal_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.wav_paths)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        wav_path = self.wav_paths[i]\n",
    "        out_path = wav_path.split(\".\")[0] + \".torch\"\n",
    "        waveform, sample_rate = torchaudio.load(wav_path)\n",
    "        waveform, orginal_len = self.padding(waveform)\n",
    "        return waveform, orginal_len, out_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def second_padding(tens: torch.tensor, orginal_lens: int) -> torch.tensor:\n",
    "    for i, len_ in enumerate(orginal_lens):\n",
    "        ratio = len_ / MAX_PADDING\n",
    "        padding_start = math.ceil(tens.shape[2] * ratio)\n",
    "        tens[i, :, padding_start:] = 0\n",
    "    return tens\n",
    "\n",
    "def save_files(tens: torch.tensor, out_paths: List[str]):\n",
    "    for i, path in enumerate(out_paths):\n",
    "        view = torch.unsqueeze(output[i], dim=0)\n",
    "        torch.save(view, path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = WavDataset(WAV_SCP_PATH)\n",
    "dataloader = DataLoader(dataset, batch_size=BATCH_SIZE, num_workers=8)\n",
    "model = load_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max_batches 1734\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "print(f\"max_batches {len(dataset) // BATCH_SIZE + 1}\")\n",
    "for i, (waves, orginal_len, out_paths) in enumerate(dataloader):\n",
    "    output = convert_wave(waves, model)\n",
    "    output = second_padding(output, orginal_len)\n",
    "    #     save_files(output, out_paths)\n",
    "    if i % 100 == 0:\n",
    "        print(i + 1)\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
